{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.find()\n",
    "\n",
    "findspark.init(\"/Users/yanji/opt/spark\")\n",
    "\n",
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"ch1\") \\\n",
    "    .config(\"spark.driver.bindAddress\",\"192.168.0.164\") \\\n",
    "    .config(\"spark.ui.port\",\"4050\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"http://{}:{}\".format(spark.conf.get(\"spark.driver.bindAddress\"), spark.conf.get(\"spark.ui.port\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\")\\\n",
    "    .csv(\"../data/flight-data/csv/2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Estonia|    1|\n",
      "|              Kosovo|      United States|    1|\n",
      "|              Zambia|      United States|    1|\n",
      "|       United States|   Papua New Guinea|    1|\n",
      "|               Malta|      United States|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|            Suriname|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|            Djibouti|      United States|    1|\n",
      "|        Burkina Faso|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|             Cyprus|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "|              Cyprus|      United States|    1|\n",
      "|       United States|          Lithuania|    1|\n",
      "|       United States|           Bulgaria|    1|\n",
      "|       United States|            Georgia|    1|\n",
      "|       United States|            Bahrain|    1|\n",
      "|       Cote d'Ivoire|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"flight_data_2015\")\n",
    "sql = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, count(1) AS count\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME ORDER BY count DESC\n",
    "\"\"\")\n",
    "dft = df.groupby(\"DEST_COUNTRY_NAME\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [destination_total#49L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(destination_total#49L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#46]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[sum(count#18)])\n",
      "         +- Exchange hashpartitioning(DEST_COUNTRY_NAME#16, 200), ENSURE_REQUIREMENTS, [id=#43]\n",
      "            +- HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[partial_sum(count#18)])\n",
      "               +- FileScan csv [DEST_COUNTRY_NAME#16,count#18] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/yanji/Documents/codes/Spark-The-Definitive-Guide/data/flig..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxsql = spark.sql('''\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY destination_total DESC\n",
    "'''\n",
    ")\n",
    "# maxsql.show()\n",
    "maxsql.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [destination_total#62L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(destination_total#62L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#66]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[sum(count#18)])\n",
      "         +- Exchange hashpartitioning(DEST_COUNTRY_NAME#16, 200), ENSURE_REQUIREMENTS, [id=#63]\n",
      "            +- HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[partial_sum(count#18)])\n",
      "               +- FileScan csv [DEST_COUNTRY_NAME#16,count#18] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/yanji/Documents/codes/Spark-The-Definitive-Guide/data/flig..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"DEST_COUNTRY_NAME\").sum(\"count\").withColumnRenamed(\"sum(count)\", \"destination_total\") \\\n",
    "    .sort('destination_total', ascending=False).explain()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d76ea728f578aa8b5ecd761146687b5718fc8d88df05df8634479f947982a73c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
