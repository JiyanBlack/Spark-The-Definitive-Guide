{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/19 12:26:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/19 12:26:02 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://192.168.0.164:4050\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "spark_home = \"~/opt/spark\"\n",
    "findspark.init(spark_home)\n",
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"ch1\") \\\n",
    "    .config(\"spark.driver.bindAddress\",\"192.168.0.164\") \\\n",
    "    .config(\"spark.ui.port\",\"4050\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"http://{}:{}\".format(spark.conf.get(\"spark.driver.bindAddress\"), spark.conf.get(\"spark.ui.port\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark-submit\n",
    "Submit codes to production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets, Type-safe structured APIs\n",
    "easier to cooperate for large projects\n",
    "not available for python and r, only with Java and Scala\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Streaming\n",
    "high-level API for steaming process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "            .load(\"../data/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.createOrReplaceTempView(\"retail_data\")\n",
    "staticSchema = sdf.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==============================================>           (8 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerId|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   17450.0|{2011-09-20 08:00...|          71601.44|\n",
      "|      null|{2011-11-14 08:00...|          55316.08|\n",
      "|      null|{2011-11-07 08:00...|          42939.17|\n",
      "|      null|{2011-03-29 08:00...| 33521.39999999998|\n",
      "|      null|{2011-12-08 08:00...|31975.590000000007|\n",
      "|   18102.0|{2011-09-15 08:00...|31661.540000000005|\n",
      "|      null|{2010-12-21 08:00...|31347.479999999938|\n",
      "|   18102.0|{2011-10-21 08:00...|          29693.82|\n",
      "|   18102.0|{2010-12-07 08:00...|          25920.37|\n",
      "|   14646.0|{2011-10-20 08:00...|25833.559999999994|\n",
      "|      null|{2010-12-10 08:00...|25399.560000000012|\n",
      "|      null|{2010-12-17 08:00...|25371.769999999768|\n",
      "|      null|{2011-11-25 08:00...|24148.069999999992|\n",
      "|      null|{2011-11-29 08:00...|23744.250000000055|\n",
      "|   12415.0|{2011-06-15 08:00...| 23426.81000000001|\n",
      "|      null|{2010-12-06 08:00...|23395.099999999904|\n",
      "|      null|{2011-08-30 08:00...| 23032.59999999993|\n",
      "|      null|{2010-12-03 08:00...| 23021.99999999999|\n",
      "|   15749.0|{2011-01-11 08:00...|           22998.4|\n",
      "|   18102.0|{2011-10-03 08:00...|          22429.69|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window, column, desc, col\n",
    "\n",
    "sdf.selectExpr(\"CustomerId\", \"(UnitPrice * Quantity) as total_cost\",\"InvoiceDate\") \\\n",
    "    .groupBy( col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\") ) \\\n",
    "        .sum(\"total_cost\").sort(desc(\"sum(total_cost)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "steamingdf = spark.readStream \\\n",
    "    .schema(staticSchema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"../data/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steamingdf.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchaseByCustomerPerhour = steamingdf \\\n",
    "    .selectExpr(\"CustomerId\", \"(UnitPrice * Quantity) as total_cost\", \"InvoiceDate\")\\\n",
    "    .groupBy(col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\")) \\\n",
    "    .sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/19 12:26:27 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/qz/3rpx4c8n1wlg5nc67fmtm2480000gn/T/temporary-be252878-9984-451a-b534-6ade7128c3dd. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/05/19 12:26:27 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x1053818e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchaseByCustomerPerhour.writeStream.format(\"memory\") \\\n",
    "    .queryName(\"customer_purchase\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerId|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   13148.0|{2010-12-10 08:00...|309.40000000000003|\n",
      "|   17368.0|{2011-01-06 08:00...| 563.1500000000001|\n",
      "|   16907.0|{2010-12-09 08:00...|182.02000000000004|\n",
      "|   15039.0|{2010-12-14 08:00...| 706.2500000000002|\n",
      "|   17519.0|{2010-12-14 08:00...|-4.949999999999999|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 642:===================================>                 (205 + 1) / 305]\r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM customer_purchase\").show(5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d76ea728f578aa8b5ecd761146687b5718fc8d88df05df8634479f947982a73c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
